{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlLk4MAAtmY4Ug5WWlo3d7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Supported-by-Hugging-Face-GPT-BERT-T5-and-other-models-available-in-the-Hugging-Face-Model-Hub./blob/main/Supported_by_Hugging_Face_GPT%2C_BERT%2C_T5%2C_and_other_models_available_in_the_Hugging_Face_Model_Hub_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Set the device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a function to load a model and tokenizer\n",
        "def load_model(model_name, for_classification=True, for_seq2seq=False):\n",
        "    if for_classification:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    elif for_seq2seq:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Define a function to classify text using a model\n",
        "def classify_text(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    return torch.argmax(logits).item()\n",
        "\n",
        "# Define a function to generate text using a model\n",
        "def generate_text(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Load the GPT-2 model and tokenizer for generation\n",
        "gpt2_model, gpt2_tokenizer = load_model(\"gpt2\", for_classification=False)\n",
        "\n",
        "# Load the BERT model and tokenizer for classification\n",
        "bert_model, bert_tokenizer = load_model(\"bert-base-uncased\")\n",
        "\n",
        "# Load the T5 model and tokenizer for translation and summarization\n",
        "t5_model, t5_tokenizer = load_model(\"t5-base\", for_classification=False, for_seq2seq=True)\n",
        "\n",
        "# Define a text to classify\n",
        "text_to_classify = \"This is a positive review.\"\n",
        "\n",
        "# Classify the text using the BERT model\n",
        "bert_output = classify_text(bert_model, bert_tokenizer, text_to_classify)\n",
        "print(\"BERT Classification:\", bert_output)\n",
        "\n",
        "# Define a prompt to generate text\n",
        "prompt_to_generate = \"Write a story about a cat.\"\n",
        "\n",
        "# Generate text using the GPT-2 model\n",
        "gpt2_output = generate_text(gpt2_model, gpt2_tokenizer, prompt_to_generate)\n",
        "print(\"GPT-2 Generation:\", gpt2_output)\n",
        "\n",
        "# Define a text to translate\n",
        "text_to_translate = \"how are you?\"\n",
        "\n",
        "# Translate the text using the T5 model\n",
        "t5_translation_output = generate_text(t5_model, t5_tokenizer, f\"translate English to French: {text_to_translate}\")\n",
        "print(\"T5 Translation:\", t5_translation_output)\n",
        "\n",
        "# Define a text to summarize\n",
        "text_to_summarize = \"Data science is the study of data that helps us derive useful insight for business decision making. Data Science is all about using tools, techniques, and creativity to uncover insights hidden within data. It combines math, computer science, and domain expertise to tackle real-world challenges in a variety of fields.\"\n",
        "\n",
        "# Summarize the text using the T5 model\n",
        "t5_summarization_output = generate_text(t5_model, t5_tokenizer, f\"summarize: {text_to_summarize}\")\n",
        "print(\"T5 Summarization:\", t5_summarization_output)\n",
        "\n",
        "# Define a text to sentiment analyze\n",
        "text_to_analyze = \"I love this product!\"\n",
        "\n",
        "# Sentiment analyze the text using the BERT model\n",
        "bert_sentiment_output = classify_text(bert_model, bert_tokenizer, text_to_analyze)\n",
        "print(\"BERT Sentiment Analysis:\", bert_sentiment_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2ju7kOYo5NJ",
        "outputId": "c3ec546d-9cad-49db-a797-7ed4bf622630"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Classification: 0\n",
            "GPT-2 Generation: Write a story about a cat.\n",
            "\n",
            "The cat is a cat.\n",
            "\n",
            "The cat is a cat.\n",
            "\n",
            "The cat\n",
            "T5 Translation: Vous êtes-vous à l'aise?\n",
            "T5 Summarization: data science is the study of data that helps us derive useful insight for business decision making .\n",
            "BERT Sentiment Analysis: 0\n"
          ]
        }
      ]
    }
  ]
}